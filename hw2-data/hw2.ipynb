{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from hw2 import get_mnist_threes_nines, display_image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12080, 28, 28) (12080,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = get_mnist_threes_nines()\n",
    "X_train, y_train = train\n",
    "X_test, y_test = test\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "rand = np.random.randint(low=0, high=X_train.shape[0])\n",
    "display_image(X_train[rand])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2a (finite differences checker, used to help implement `my_nn_finite_difference_checker` in 1.3a. Feel free to modify the function signature, or to skip this part and implement `my_nn_finite_difference_checker` without this helper function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_difference_checker(f, x, k):\n",
    "    \"\"\"Returns \\frac{\\partial f}{\\partial x_k}(x)\"\"\"\n",
    "    epsilon = 10**-5\n",
    "    epsilon_vector = np.zeros(x.shape)\n",
    "    epsilon_vector[k] = epsilon\n",
    "    derivative = (f(x + epsilon_vector)[0] - f(x - epsilon_vector)[0])/2*epsilon\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2b (functions that implement neural network layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "\n",
    "    epsilon = 10**-15\n",
    "\n",
    "    if s<0:\n",
    "        return np.clip(np.exp(s)/(1+np.exp(s)), epsilon, 1-epsilon)\n",
    "    else:\n",
    "        return np.clip(1/(1+np.exp(-s)), epsilon, 1-epsilon)\n",
    "\n",
    "def sigmoid_derivative(s):\n",
    "    return sigmoid(s)*(1-sigmoid(s))\n",
    "\n",
    "def sigmoid_activation(x):\n",
    "\n",
    "    sigmoid_vectorized = np.vectorize(sigmoid)\n",
    "    out = sigmoid_vectorized(x)\n",
    "\n",
    "    sigmoid_derivative_vectorized = np.vectorize(sigmoid_derivative)\n",
    "    grad = sigmoid_derivative_vectorized(x)\n",
    "    assert grad.shape == x.shape\n",
    "    assert out.shape == x.shape\n",
    "    return out, grad\n",
    "\n",
    "def logistic_loss(g, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for binary classification with logistic\n",
    "    loss\n",
    "\n",
    "    Inputs:\n",
    "    - g: Output of final layer with sigmoid activation,\n",
    "         of shape (n, 1)\n",
    "\n",
    "    - y: Vector of labels, of shape (n,) where y[i] is the label for x[i]\n",
    "         and y[i] in {0, 1}\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: array of losses\n",
    "    - dL_dg: Gradient of the loss with respect to g\n",
    "    \"\"\"\n",
    "    loss = -(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "    dL_dg = (g-y)/(g-g**2)\n",
    "    return loss, dL_dg\n",
    "\n",
    "def relu(s):\n",
    "    return max(0,s)\n",
    "\n",
    "def relu_derivative(s):\n",
    "    return int(s>0)\n",
    "\n",
    "def relu_activation(s):\n",
    "    relu_vectorized = np.vectorize(relu)\n",
    "    out = relu_vectorized(s)\n",
    "\n",
    "    relu_derivative_vectorized = np.vectorize(relu_derivative)\n",
    "    ds = relu_derivative_vectorized(s)\n",
    "    assert ds.shape == s.shape\n",
    "    assert out.shape == s.shape\n",
    "    return out, ds\n",
    "\n",
    "def layer_forward(x, W, b, activation_fn):\n",
    "    bias = np.repeat(b, x.shape[0], axis = 0)\n",
    "    S_l = (x @ W) + bias\n",
    "    out, gradient = activation_fn(S_l)\n",
    "\n",
    "\n",
    "    #Need delta_j at layer l, weights at layer l and the derivative\n",
    "    #of activation function at S_i at layer l-1\n",
    "    cache = np.array([out, gradient, W])\n",
    "\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3b i, ii (deliverables for the sigmoid activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73 0.5  0.27]\n",
      "[0.2  0.25 0.2 ]\n",
      "================================================================================\n",
      "[1.e-15 1.e+00]\n",
      "[1.00000000e-15 9.99200722e-16]\n"
     ]
    }
   ],
   "source": [
    "# 1.3b i\n",
    "s = np.asarray([1., 0., -1])\n",
    "out, grad = sigmoid_activation(s)\n",
    "with np.printoptions(precision=2):\n",
    "    print(out)\n",
    "    print(grad)\n",
    "    \n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1.3b ii\n",
    "s = np.asarray([-1000., 1000.])\n",
    "out, grad = sigmoid_activation(s)\n",
    "print(out)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.3 ii\n",
    "We'll get an error if the exp() function takes in a too negative number. I therefore switch between two different equations (but yielding same number) for the sigmoid to avoid having exp() of a large number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3b iii: What is the derivative of the negative log-likelihood loss with respect to $g$?\n",
    "\n",
    "The derivative is $\\frac{g-y}{g-g^2}$.\n",
    "\n",
    "1.3b iv: Explain what is returned in `cache` in your `layer_forward` implementation. (Trying to answer this question before completing your implementation might help think about should go in `cache`, which should be stuff computed during the forward pass that is needed for backpropagation in the backward pass. Just make sure your final answer pertains to what you ultimately return in `cache`.)\n",
    "\n",
    "\n",
    "- Output of the layer to use when calculating $\\frac{\\partial L}{\\partial w_{ij}}$ using $\\delta_j$\n",
    "- The gradient of the output given $S_l$ used when calculating $delta_j$\n",
    "- The weights from layer i to j, used to calculate $delta_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2c (in this part you will code functions that initialize the neural network's weights. You will also code the forward pass which ties everything together, computing the output of a neural network with weights given by `weight_matrices` + `biases`, activation functions given by `activations`, on the input `X_batch`, a 2d input where each row is an individual input vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weight_matrices(layer_dims):\n",
    "    \"\"\"\n",
    "    Creates a list of weight matrices defining the weights of NN\n",
    "    \n",
    "    Inputs:\n",
    "    - layer_dims: A list whose size is the number of layers. layer_dims[i] defines\n",
    "      the number of neurons in the i+1 layer.\n",
    "\n",
    "    Returns a list of weight matrices\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for i in range(len(layer_dims)-1):\n",
    "        weights.append(np.full((layer_dims[i], layer_dims[i+1]), np.random.normal(loc=0.0, scale= 0.01, size = (layer_dims[i], layer_dims[i+1]))))\n",
    "    return weights\n",
    "\n",
    "def create_bias_vectors(layer_dims):\n",
    "    biases = []\n",
    "    for i in range(len(layer_dims)-1):\n",
    "        biases.append(np.full((1, layer_dims[i+1]), np.random.normal(loc=0.0, scale = 0.01, size = (1, layer_dims[i+1]))))\n",
    "    return biases\n",
    "\n",
    "def forward_pass(X_batch, weight_matrices, biases, activations):\n",
    "    layer_caches = [[X_batch]]\n",
    "    outs = []\n",
    "    output = X_batch\n",
    "    for i in range(len(weight_matrices)):\n",
    "        output, cache = layer_forward(output, weight_matrices[i], biases[i], activations[i])\n",
    "        outs.append(output)\n",
    "        layer_caches.append(cache)\n",
    "\n",
    "    output = outs[-1]\n",
    "    return output, layer_caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3c (deliverable which has you run a forward pass of your neural network and compute its logistic loss on some output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6985204992027605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elias\\AppData\\Local\\Temp\\ipykernel_26764\\2568116894.py:68: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  cache = np.array([out, gradient, W])\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_batch_weights_biases.pkl\", \"rb\") as fn:\n",
    "    (X_batch, y_batch, weight_matrices, biases) = pickle.load(fn)\n",
    "activations = [relu_activation, sigmoid_activation]\n",
    "output, layer_caches = forward_pass(X_batch, weight_matrices, biases,\n",
    "                         activations)\n",
    "loss, dL_dg = logistic_loss(output, y_batch)\n",
    "print(loss.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3a (deliverable which has you compute the gradient w.r.t. `weight_matrices` and `biases` using a finite differences checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "[0.0, 0.0]\n",
      "\n",
      "[-0.5026794540896606]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elias\\AppData\\Local\\Temp\\ipykernel_26764\\2568116894.py:68: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  cache = np.array([out, gradient, W])\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_batch_weights_biases.pkl\", \"rb\") as fn:\n",
    "    (X_batch, y_batch, weight_matrices, biases) = pickle.load(fn)\n",
    "\n",
    "def my_nn_finite_difference_checker(X_batch, y_batch, weight_matrices, biases, activations):\n",
    "    output =  forward_pass(X_batch, weight_matrices, biases, activations)[0]\n",
    "\n",
    "    epsilon = 1E-5\n",
    "    grad_Ws = []\n",
    "    grad_bs = []\n",
    "\n",
    "    for layer_index in range(len(weight_matrices)):\n",
    "        layer_weight_difs = np.zeros(weight_matrices[layer_index].shape)\n",
    "        layer_bias_difs = []\n",
    "\n",
    "        for c in range(len(weight_matrices[layer_index])):\n",
    "            for r in range(len(weight_matrices[layer_index][c])):\n",
    "                weight_matrices[layer_index][c, r] += epsilon\n",
    "                large = logistic_loss(forward_pass(X_batch, weight_matrices, biases, activations)[0], y_batch)[0]\n",
    "\n",
    "                weight_matrices[layer_index][c, r] -= 2*epsilon\n",
    "                small = logistic_loss(forward_pass(X_batch, weight_matrices, biases, activations)[0], y_batch)[0]\n",
    "\n",
    "                weight_matrices[layer_index][c, r] += epsilon\n",
    "\n",
    "                dif = np.average((large - small) / (2*epsilon))\n",
    "                layer_weight_difs[c, r] = dif\n",
    "\n",
    "        for bias_index in range(len(biases[layer_index][0])):\n",
    "            biases[layer_index][0][bias_index] -= epsilon\n",
    "            small = logistic_loss(forward_pass(X_batch, weight_matrices, biases, activations)[0], y_batch)[0]\n",
    "\n",
    "            biases[layer_index][0][bias_index] += 2*epsilon\n",
    "            large = logistic_loss(forward_pass(X_batch, weight_matrices, biases, activations)[0], y_batch)[0]\n",
    "\n",
    "            biases[layer_index][0][bias_index] -= epsilon\n",
    "\n",
    "            dif = np.average((large - small) / (2*epsilon))\n",
    "            layer_bias_difs.append(dif)\n",
    "\n",
    "        grad_Ws.append(layer_weight_difs)\n",
    "        grad_bs.append(layer_bias_difs)\n",
    "\n",
    "    return grad_Ws, grad_bs\n",
    "\n",
    "grad_Ws, grad_bs = my_nn_finite_difference_checker(X_batch, y_batch, weight_matrices, biases, activations)\n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print(grad_Ws[0])\n",
    "    print()\n",
    "    print(grad_Ws[1])\n",
    "    print()\n",
    "    print(grad_bs[0])\n",
    "    print()\n",
    "    print(grad_bs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2d (the backward pass!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(dL_dg, layer_caches):\n",
    "    grad_Ws = []\n",
    "    grad_bs = []\n",
    "\n",
    "    nr_of_layers = len(layer_caches)\n",
    "    deltas = [None] * nr_of_layers\n",
    "\n",
    "    dgds_L = np.array(layer_caches[-1][1]).flatten()\n",
    "    delta_L = np.reshape(dL_dg * dgds_L, (-1, 1))\n",
    "\n",
    "    deltas[-1] = delta_L\n",
    "\n",
    "    #Compute delta s^l for other layers\n",
    "    for l in reversed(range(1, nr_of_layers - 1)):\n",
    "        out, dg_dsLm1, _ = layer_caches[l]\n",
    "        W = layer_caches[l + 1][2]\n",
    "\n",
    "        # Compute delta for layer below\n",
    "        delta_L = deltas[l + 1]\n",
    "        delta_l = dg_dsLm1  * (delta_L@W.T)\n",
    "        deltas[l] = delta_l\n",
    "\n",
    "\n",
    "    #grad Ws\n",
    "    for l in range(nr_of_layers - 1):\n",
    "        out = layer_caches[l][0]\n",
    "        out_mat = np.average(out, axis=0).reshape((-1, 1))\n",
    "        delta_mat = np.average(deltas[l + 1], axis=0).reshape((1, -1))\n",
    "        dl_dW = out_mat @ delta_mat\n",
    "        grad_Ws.append(dl_dW)\n",
    "\n",
    "    #grad bs\n",
    "    for delta in deltas:\n",
    "        if delta is not None:\n",
    "            grad_bs.append(np.average(delta, axis=0))\n",
    "\n",
    "    return grad_Ws, grad_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3d (test your backward pass! compare it with 1.3a, the gradient computed by the finite difference checker. The answers should match!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elias\\AppData\\Local\\Temp\\ipykernel_26764\\2568116894.py:68: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  cache = np.array([out, gradient, W])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,2) (4,2) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [30], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m output, layer_caches \u001B[38;5;241m=\u001B[39m forward_pass(X_batch, weight_matrices, biases,\n\u001B[0;32m      6\u001B[0m                                     activations)\n\u001B[0;32m      7\u001B[0m loss, dL_dg \u001B[38;5;241m=\u001B[39m logistic_loss(output, y_batch)\n\u001B[1;32m----> 8\u001B[0m grad_Ws, grad_bs \u001B[38;5;241m=\u001B[39m \u001B[43mbackward_pass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdL_dg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_caches\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m grad_Ws_fd, grad_bs_fd \u001B[38;5;241m=\u001B[39m my_nn_finite_difference_checker(X_batch,\n\u001B[0;32m     11\u001B[0m                                                    y_batch,\n\u001B[0;32m     12\u001B[0m                                                    weight_matrices,\n\u001B[0;32m     13\u001B[0m                                                    biases,\n\u001B[0;32m     14\u001B[0m                                                    activations)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m np\u001B[38;5;241m.\u001B[39mprintoptions(precision\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m):\n",
      "Cell \u001B[1;32mIn [29], line 20\u001B[0m, in \u001B[0;36mbackward_pass\u001B[1;34m(dL_dg, layer_caches)\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;66;03m# Compute delta for layer below\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     delta_L \u001B[38;5;241m=\u001B[39m deltas[l \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m---> 20\u001B[0m     delta_l \u001B[38;5;241m=\u001B[39m \u001B[43mdg_dsLm1\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelta_L\u001B[49m\u001B[38;5;129;43m@np\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m     deltas[l] \u001B[38;5;241m=\u001B[39m delta_l\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m#grad Ws\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (2,2) (4,2) "
     ]
    }
   ],
   "source": [
    "with open(\"test_batch_weights_biases.pkl\", \"rb\") as fn:\n",
    "    (X_batch, y_batch, weight_matrices, biases) = pickle.load(fn)\n",
    "\n",
    "activations = [relu_activation, sigmoid_activation]\n",
    "output, layer_caches = forward_pass(X_batch, weight_matrices, biases,\n",
    "                                    activations)\n",
    "loss, dL_dg = logistic_loss(output, y_batch)\n",
    "grad_Ws, grad_bs = backward_pass(dL_dg, layer_caches)\n",
    "\n",
    "grad_Ws_fd, grad_bs_fd = my_nn_finite_difference_checker(X_batch,\n",
    "                                                   y_batch,\n",
    "                                                   weight_matrices,\n",
    "                                                   biases,\n",
    "                                                   activations)\n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print(grad_Ws[0])\n",
    "    print(grad_Ws_fd[0])\n",
    "    print()\n",
    "    print(grad_Ws[1])\n",
    "    print(grad_Ws_fd[1])\n",
    "    print()\n",
    "    print(grad_bs[0])\n",
    "    print(grad_bs_fd[0])\n",
    "    print()\n",
    "    print(grad_bs[1])\n",
    "    print(grad_bs_fd[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2e (train your neural network on MNIST! save the training and test losses and accuracies at each iteration to use in 1.3e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = get_mnist_threes_nines()\n",
    "layer_dims = [784, 200, 1]\n",
    "activations = [relu_activation, sigmoid_activation]\n",
    "weights = create_weight_matrices(layer_dims)\n",
    "biases = create_bias_vectors([200, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3e code answers for i, ii, iii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i\n",
    "# Plot the train and test losses from the MNIST network with step size = 0.1\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# ii\n",
    "# Plot the train and test accuracies from the MNIST network with step size = 0.1\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# iii\n",
    "# Visualize (plot) some images that are misclassified by your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3e iii:\n",
    "Examine the images that your network guesses incorrectly, and explain at a high level\n",
    "what patterns you see in those images.\n",
    "\n",
    "**your answer here**\n",
    "\n",
    "1.3e iv:\n",
    "Rerun the neural network training but now increase the step size to 10.0. What happens?\n",
    "You do not need to include plots here.\n",
    "\n",
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3f (optional) (Train a network to fit 100 random images to the first 100 original labels! How fast can you memorize the dataset?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.rand(100, 784)\n",
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}